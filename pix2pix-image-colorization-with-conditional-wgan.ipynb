{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/salimhammadi07/pix2pix-image-colorization-with-conditional-wgan?scriptVersionId=129534035\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"!pip install torchsummary","metadata":{"execution":{"iopub.status.busy":"2023-01-26T09:01:22.903949Z","iopub.execute_input":"2023-01-26T09:01:22.904948Z","iopub.status.idle":"2023-01-26T09:01:32.43879Z","shell.execute_reply.started":"2023-01-26T09:01:22.904904Z","shell.execute_reply":"2023-01-26T09:01:32.437249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfrom pathlib import Path\n\n# Import glob to get the files directories recursively\nimport glob\n\n# Import Garbage collector interface\nimport gc \n\n# Import OpenCV to transforme pictures\nimport cv2\n\n# Import Time\nimport time\n\n# import numpy for math calculations\nimport numpy as np\n\n# Import pandas for data (csv) manipulation\nimport pandas as pd\n\n# Import matplotlib for plotting\nimport matplotlib.pyplot as plt\nimport matplotlib\nmatplotlib.style.use('fivethirtyeight') \n%matplotlib inline\n\nimport PIL\nfrom PIL import Image\nfrom skimage.color import rgb2lab, lab2rgb\n\nimport pytorch_lightning as pl\n\n# Import pytorch to build Deel Learling Models \nimport torch\nfrom torch import nn, optim\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.autograd import Variable\nfrom torchvision import models\nfrom torch.nn import functional as F\nimport torch.utils.data\nfrom torchvision.models.inception import inception_v3\nfrom scipy.stats import entropy\n\nfrom torchsummary import summary\n\n# Import tqdm to show a smart progress meter\nfrom tqdm import tqdm\n\n# Import warnings to hide the unnessairy warniings\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-01-26T09:01:32.442187Z","iopub.execute_input":"2023-01-26T09:01:32.442607Z","iopub.status.idle":"2023-01-26T09:01:32.457629Z","shell.execute_reply.started":"2023-01-26T09:01:32.442562Z","shell.execute_reply":"2023-01-26T09:01:32.456446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<hr>\n<h1><center>I. Project Understanding</center></h1>\n<hr>","metadata":{}},{"cell_type":"markdown","source":"## A. Introduction","metadata":{}},{"cell_type":"markdown","source":"\n<div class=\"alert alert-block alert-info\" style=\"font-size:14px; color:black; background-color:#e6f2ff; border-color: #3399ff\">\n    <aside>\n    üìò Pix2pix is a powerful model for image-to-image translation tasks, but it can be further improved for specific applications such as colorization. One way to improve the performance of pix2pix for colorization is to use a Wasserstein GAN (WGAN) instead of the traditional GAN architecture. WGANs use the Wasserstein distance metric to train the generator and discriminator, which can help stabilize the training process and produce more realistic results.\n\nAnother way to improve the performance of pix2pix for colorization is to use a U-Net architecture based on residual blocks. U-Net is a type of convolutional neural network (CNN) that is well-suited for image segmentation tasks. It consists of a series of convolutional layers and max pooling layers, with skip connections between layers of the same resolution. This allows the network to learn fine details of the input image, which can be particularly useful for colorization tasks.\n\nResidual blocks are a type of building block for neural networks, which consist of multiple layers with skip connections. The skip connections allow the gradient to pass through the layers more easily, which can help the network converge faster and produce better results.\n\nUsing WGAN with a U-Net architecture based on residual blocks can help improve the performance of pix2pix for colorization by providing better stability and improved ability to learn fine details of the input image.\n    </aside>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<aside>\nüìå The goal of this paper is :\n\n- The goal of the \"Image-to-Image Translation with Conditional Adversarial Networks\" (pix2pix) paper is to propose a method for image-to-image translation using a conditional GAN architecture.\n- The method uses a generator network that is trained to convert images from one domain (e.g. sketches) to another domain (e.g. photographs).\n- The generator is trained using a combination of adversarial loss and L1 loss.\n- The generator takes an image from the input domain and a random noise as input, and generates an image in the target domain.\n- The discriminator network is trained to classify the generated image as real or fake, based on whether it is similar to a target image from the target domain.\n- The paper uses the patch GAN architecture to discriminator with 70x70 patches.\n- The authors showed the effectiveness of the proposed method on a variety of image-to-image translation tasks, such as converting edges to photographs, day to night, and labels to street scenes.\n</aside>","metadata":{}},{"cell_type":"markdown","source":"<aside>\n‚ùì What is image colorization ? \n  <br> <br/>\nImage colorization is the process of adding color to a grayscale image or a black and white image. It involves mapping the intensity values of the grayscale image to a color space, such as RGB, and then filling in the missing color channels to produce a full-color image. There are different ways to approach image colorization, but most methods involve some form of image processing, such as image segmentation, texture synthesis, or machine learning.\n\nOne popular approach is to use a deep learning-based method, such as a convolutional neural network (CNN) to colorize images. This approach typically involves training a CNN on a large dataset of color images, and then using this network to predict the missing color channels of a grayscale image.\n\nAnother approach is to use a Generative Adversarial Network (GAN) model, where a generator network generates the color version of the grayscale image and a discriminator network is trained to distinguish between the generated color version and the real color image.\n\nIn recent years, there have been some impressive results in image colorization using deep learning-based methods, which can produce high-quality colorization results on a wide range of images.\n</aside>","metadata":{}},{"cell_type":"markdown","source":"## B. Theory ","metadata":{}},{"cell_type":"markdown","source":"### 1. Generative adversarial networks (GAN)","metadata":{}},{"cell_type":"markdown","source":"A generative adversarial network (GAN) is a type of deep learning network that can generate data with similar characteristics as the input training data.\n\nA GAN consists of two networks that train together:\n\n* Generator ‚Äî Given a vector of random values as input, this network generates data with the same structure as the training data.\n\n* Discriminator ‚Äî Given batches of data containing observations from both the training data, and generated data from the generator, this network attempts to classify the observations as \"real\" or \"generated\".\n\n<center><img src=\"https://it.mathworks.com/help/examples/nnet/win64/TrainConditionalGenerativeAdversarialNetworkCGANExample_01.png\"/> </center>","metadata":{}},{"cell_type":"markdown","source":"### 2. Conditional Generative adversarial networks (cGAN)\n","metadata":{}},{"cell_type":"markdown","source":"A conditional generative adversarial network (CGAN) is a type of GAN that also takes advantage of labels during the training process.\n\n* Generator ‚Äî Given a label and random array as input, this network generates data with the same structure as the training data observations corresponding to the same label.\n\n* Discriminator ‚Äî Given batches of labeled data containing observations from both the training data and generated data from the generator, this network attempts to classify the observations as \"real\" or \"generated\".\n\n<center><img src=\"https://it.mathworks.com/help/examples/nnet/win64/TrainConditionalGenerativeAdversarialNetworkCGANExample_02.png\"/> </center>\n\nTo train a conditional GAN, train both networks simultaneously to maximize the performance of both:\n\n* Train the generator to generate data that \"fools\" the discriminator.\n\n* Train the discriminator to distinguish between real and generated data.\n\nTo maximize the performance of the generator, maximize the loss of the discriminator when given generated labeled data. That is, the objective of the generator is to generate labeled data that the discriminator classifies as \"real\".\n\nTo maximize the performance of the discriminator, minimize the loss of the discriminator when given batches of both real and generated labeled data. That is, the objective of the discriminator is to not be \"fooled\" by the generator.\n\nIdeally, these strategies result in a generator that generates convincingly realistic data that corresponds to the input labels and a discriminator that has learned strong feature representations that are characteristic of the training data for each label.\n\nsource : https://it.mathworks.com/help/deeplearning/ug/train-conditional-generative-adversarial-network.html","metadata":{}},{"cell_type":"markdown","source":"### 3. Why choosing cGAN over GAN\n\nConditional Generative Adversarial Networks (CGANs) are an extension of standard Generative Adversarial Networks (GANs) that are designed to handle conditional data. A CGAN consists of a generator network and a discriminator network, just like a standard GAN. However, in a CGAN, the generator and discriminator are both conditioned on some additional input data. This additional input data can be used to control the output of the generator, allowing it to produce more specific or customized results.\n\nThere are several reasons why a CGAN can be better than a standard GAN:\n\n1. Control over the generated data: In a CGAN, the generator's output is conditioned on the input data, which allows the model to be more specific and controlled in its output. For example, if the input is a grayscale image, the model can colorize it to a specific color scheme.\n\n2. Improved stability and training: Because the generator is conditioned on additional input data, it can be easier to train and more stable than a standard GAN. This is because the generator is able to focus on a specific subset of the data, rather than trying to generate all possible outputs.\n\n3. Handling missing data: CGANs are well suited for handling missing data or data with missing modalities. The additional input data can be used to condition the generator to produce plausible outputs for the missing data.\n\n4. Handling multiple classes: CGANs can be used to generate data for multiple classes in a one-to-many mapping, where the generator is conditioned on the class label and produces an image from that class.\n\n5. Handling conditional data: In some tasks, the data is conditional, such as in image-to-image translation, where the output is conditioned on the input. CGANs can handle this kind of conditional data very well.\n\nIt's important to note that in some tasks a GAN might be enough or even better than a CGAN, it depends on the task and the data.","metadata":{}},{"cell_type":"markdown","source":"<hr>\n<h1><center>II. Data Preparation</center></h1>\n<hr>","metadata":{}},{"cell_type":"code","source":"ab_path = \"/kaggle/input/image-colorization/ab/ab/ab1.npy\"\nl_path = \"/kaggle/input/image-colorization/l/gray_scale.npy\"","metadata":{"execution":{"iopub.status.busy":"2023-01-26T09:01:32.459134Z","iopub.execute_input":"2023-01-26T09:01:32.459488Z","iopub.status.idle":"2023-01-26T09:01:32.470588Z","shell.execute_reply.started":"2023-01-26T09:01:32.459452Z","shell.execute_reply":"2023-01-26T09:01:32.469574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ab_df = np.load(ab_path)[0:3000]\nL_df = np.load(l_path)[0:3000]\ndataset = (L_df,ab_df )\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-01-26T09:01:32.475014Z","iopub.execute_input":"2023-01-26T09:01:32.475306Z","iopub.status.idle":"2023-01-26T09:01:44.012038Z","shell.execute_reply.started":"2023-01-26T09:01:32.475281Z","shell.execute_reply":"2023-01-26T09:01:44.010986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<hr>\n<h1><center>III. Data Exploration and Visualiaztion</center></h1>\n<hr>","metadata":{}},{"cell_type":"markdown","source":"## A. L*a*b* Colors\n\nLike geographic coordinates ‚Äì longitude, latitude, and altitude ‚Äì L*a*b* color values give us a way to locate and communicate colors.\n\nIn the 1940‚Äôs, Richard Hunter introduced a tri-stimulus model, Lab, which is scaled to achieve near uniform spacing of perceived color differences. While Hunter‚Äôs Lab was adopted as the de facto model for plotting absolute color coordinates and differences between colors, it was never formally accepted as an international standard. \n\n### What does L*a*b* stand for? \n it‚Äôs important to know what L*, a*, and b*stand for. \n\n* L*: Lightness\n* a*: Red/Green Value\n* b*: Blue/Yellow Value\n\nAs an example,  showing the color-plotting diagrams for L*a*b*.\n<center>\n<img src=\"https://www.xrite.com/-/media/modules/weblog/blog/lab-color-space/lab-color-space.png?h=622&w=600&la=en&hash=53A76941BAB3015346FAB3689739E967843CF8EA\"></center>\n\n* The a* axis runs from left to right. A color measurement movement in the +a direction depicts a shift toward red.\n* Along the b* axis, +b movement represents a shift toward yellow.\n* The center L* axis shows L = 0 (black or total absorption) at the bottom.\n* At the center of this plane is neutral or gray.\n\nSource : https://www.xrite.com/blog/lab-color-space\n\n### Why chosing L*a*b* for our problem ?\n\nsing the LAB color space for image colorization can be beneficial for several reasons:\n\n1. LAB color space is perceptually uniform: The LAB color space separates color information (A and B channels) from lightness information (L channel), which allows for more accurate color representation. This can be particularly useful for image colorization, as it allows for more precise control over the colorization process.\n\n2. LAB color space is more suitable for image processing: LAB color space is designed to be perceptually uniform and it separates the lightness information from the color information. This makes it more suitable for image processing tasks such as colorization, where it is important to maintain the relationship between lightness and color.\n\n3. LAB color space is more robust to lighting changes: The L channel of the LAB color space represents the lightness of the image, which is relatively robust to changes in lighting conditions. This can be useful when colorizing images taken under different lighting conditions, as it allows for more consistent results.\n\n4. LAB color space is more similar to human vision: The LAB color space is based on the way human eyes perceive color, which means that the results of colorization in LAB space are more similar to what a human would perceive.\n\n5. LAB color space can be converted to other color spaces: LAB color space can be easily converted to other color spaces such as RGB which is the most common color space used in computer vision and image processing.\n\n6. To train a model for colorization, we should give it a grayscale image and hope that it will make it colorful. When using L*a*b, we can give the L channel to the model (which is the grayscale image) and want it to¬†predict the other two channels¬†(*a, *b) and after its prediction, we concatenate all the channels and we get our colorful image. But if you use RGB, you have to¬†first convert your image to grayscale, feed the grayscale image to the model and hope¬†it will predict 3 numbers¬†for you which is a way more difficult and unstable task due to the many more possible combinations of 3 numbers compared to two numbers. \n\nIt's important to note that using LAB color space is not the only option for image colorization, and other color spaces such as RGB can also be used. But LAB color space has been proved to be a better option for image colorization due to the above-mentioned reasons.","metadata":{}},{"cell_type":"code","source":"def lab_to_rgb(L, ab):\n    \"\"\"\n    Takes an image or a batch of images and converts from LAB space to RGB\n    \"\"\"\n    L = L  * 100\n    ab = (ab - 0.5) * 128 * 2\n    Lab = torch.cat([L, ab], dim=2).numpy()\n    rgb_imgs = []\n    for img in Lab:\n        img_rgb = lab2rgb(img)\n        rgb_imgs.append(img_rgb)\n    return np.stack(rgb_imgs, axis=0)","metadata":{"execution":{"iopub.status.busy":"2023-01-26T09:01:44.013621Z","iopub.execute_input":"2023-01-26T09:01:44.013998Z","iopub.status.idle":"2023-01-26T09:01:44.019838Z","shell.execute_reply.started":"2023-01-26T09:01:44.013961Z","shell.execute_reply":"2023-01-26T09:01:44.018948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(30,30))\nfor i in range(1,16,2):\n    plt.subplot(4,4,i)\n    img = np.zeros((224,224,3))\n    img[:,:,0] = L_df[i]\n    plt.title('B&W')\n    plt.imshow(lab2rgb(img))\n    \n    plt.subplot(4,4,i+1)\n    img[:,:,1:] = ab_df[i]\n    img = img.astype('uint8')\n    img = cv2.cvtColor(img, cv2.COLOR_LAB2RGB)\n    plt.title('Colored')\n    plt.imshow(img)","metadata":{"execution":{"iopub.status.busy":"2023-01-26T09:01:44.021414Z","iopub.execute_input":"2023-01-26T09:01:44.022285Z","iopub.status.idle":"2023-01-26T09:01:47.415104Z","shell.execute_reply.started":"2023-01-26T09:01:44.022247Z","shell.execute_reply":"2023-01-26T09:01:47.413189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-01-26T09:01:47.4203Z","iopub.execute_input":"2023-01-26T09:01:47.42097Z","iopub.status.idle":"2023-01-26T09:01:47.6718Z","shell.execute_reply.started":"2023-01-26T09:01:47.42093Z","shell.execute_reply":"2023-01-26T09:01:47.670936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<hr>\n<h1><center>III. Data Loader</center></h1>\n<hr>","metadata":{}},{"cell_type":"code","source":"class ImageColorizationDataset(Dataset):\n    ''' Black and White (L) Images and corresponding A&B Colors'''\n    def __init__(self, dataset, transform=None):\n        '''\n        :param dataset: Dataset name.\n        :param data_dir: Directory with all the images.\n        :param transform: Optional transform to be applied on sample\n        '''\n        self.dataset = dataset\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.dataset[0])\n    \n    def __getitem__(self, idx):\n        L = np.array(dataset[0][idx]).reshape((224,224,1))\n        L = transforms.ToTensor()(L)\n        \n        ab = np.array(dataset[1][idx])\n        ab = transforms.ToTensor()(ab)\n\n        return ab, L","metadata":{"execution":{"iopub.status.busy":"2023-01-26T09:01:47.673748Z","iopub.execute_input":"2023-01-26T09:01:47.674515Z","iopub.status.idle":"2023-01-26T09:01:47.684237Z","shell.execute_reply.started":"2023-01-26T09:01:47.674467Z","shell.execute_reply":"2023-01-26T09:01:47.683286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 1\n\n# Prepare the Datasets\ntrain_dataset = ImageColorizationDataset(dataset = (L_df, ab_df))\ntest_dataset = ImageColorizationDataset(dataset = (L_df, ab_df))\n\n# Build DataLoaders\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle = True, pin_memory = True)\ntest_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle = False, pin_memory = True)","metadata":{"execution":{"iopub.status.busy":"2023-01-26T09:01:47.685712Z","iopub.execute_input":"2023-01-26T09:01:47.686319Z","iopub.status.idle":"2023-01-26T09:01:47.696535Z","shell.execute_reply.started":"2023-01-26T09:01:47.686282Z","shell.execute_reply":"2023-01-26T09:01:47.695534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<hr>\n<h1><center>IV. Data Modelling</center></h1>\n<hr>","metadata":{}},{"cell_type":"markdown","source":"# A. Generator ( ResU-NET )\n>UNet with ResBlock for Semantic Segmentation","metadata":{}},{"cell_type":"markdown","source":"## 1. Theory","metadata":{}},{"cell_type":"markdown","source":"The U-Net is a convolutional neural network architecture that is designed for fast and precise segmentation of images. It has performed extremely well in several challenges and to this day, it is one of the most popular end-to-end architectures in the field of semantic segmentation.\n\nWe can split the network into two parts: \n* The encoder path (backbone) and the decoder path. The encoder captures features at different scales of the images by using a traditional stack of convolutional and max pooling layers.Concretely speaking, a block in the encoder consists of the repeated use of two convolutional layers (k=3, s=1), each followed by a non-linearity layer, and a max-pooling layer (k=2, s=2). For every convolution block and its associated max pooling operation, the number of feature maps is doubled to ensure that the network can learn the complex structures effectively.\n\n* The decoder path is a symmetric expanding counterpart that uses transposed convolutions. This type of convolutional layer is an up-sampling method with trainable parameters and performs the reverse of (down)pooling layers such as the max pool. Similar to the encoder, each convolution block is followed by such an up-convolutional layer. The number of feature maps is halved in every block. Because recreating a segmentation mask from a small feature map is a rather difficult task for the network, the output after every up-convolutional layer is appended by the feature maps of the corresponding encoder block. The feature maps of the encoder layer are cropped if the dimensions exceed the one of the corresponding decoder layers.\n\nSource : https://towardsdatascience.com/creating-and-training-a-u-net-model-with-pytorch-for-2d-3d-semantic-segmentation-model-building-6ab09d6a0862","metadata":{}},{"cell_type":"markdown","source":"## 2. UNet with ResBlock for Semantic Segmentation\nUNet architecture was a great step forward in computer vision that revolutionized segmentation not just in medical imaging but in other fields as well. The long skip connection between each level of contracting path and expanding path is the key feature of the UNet. It‚Äôs like FCN is pulled upwards from both ends.\n\nAnother revolutionary advancement in computer vision was ResNet. The residual blocks in ResNet with skip connections helped in making a deeper and deeper convolution neural network and achieved record-breaking results for classification on the ImageNet dataset.\n<center> <img src=\"https://miro.medium.com/max/720/0*Q6Dq_Ztsno3zV8TF\"> </center>\n\nNow by replacing convolutions in U-Net on each level with ResBlock, we can get better performance than the original UNet almost every time. Below is the detailed model architecture diagram.\n\nsource  : https://medium.com/@nishanksingla/unet-with-resblock-for-semantic-segmentation-dd1766b4ff66","metadata":{}},{"cell_type":"markdown","source":"## 3. Achitecture ","metadata":{}},{"cell_type":"markdown","source":"<center> <img src=\"https://i.imgur.com/k6ErEni.png\"></center","metadata":{}},{"cell_type":"markdown","source":"## 4. Implementation","metadata":{}},{"cell_type":"code","source":"class ResBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1):\n        super().__init__()\n        self.layer = nn.Sequential(\n            nn.Conv2d(in_channels,out_channels,kernel_size=3, padding=1, stride=stride, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels,kernel_size=3,padding=1, stride=1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n\n        self.identity_map = nn.Conv2d(in_channels, out_channels,kernel_size=1,stride=stride)\n        self.relu = nn.ReLU(inplace=True)\n    def forward(self, inputs):\n        x = inputs.clone().detach()\n        out = self.layer(x)\n        residual  = self.identity_map(inputs)\n        skip = out + residual\n        return self.relu(skip)","metadata":{"execution":{"iopub.status.busy":"2023-01-26T09:01:47.70159Z","iopub.execute_input":"2023-01-26T09:01:47.702214Z","iopub.status.idle":"2023-01-26T09:01:47.711162Z","shell.execute_reply.started":"2023-01-26T09:01:47.702176Z","shell.execute_reply":"2023-01-26T09:01:47.710104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DownSampleConv(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1):\n        super().__init__()\n        self.layer = nn.Sequential(\n            nn.MaxPool2d(2),\n            ResBlock(in_channels, out_channels)\n        )\n\n    def forward(self, inputs):\n        return self.layer(inputs)","metadata":{"execution":{"iopub.status.busy":"2023-01-26T09:01:47.714232Z","iopub.execute_input":"2023-01-26T09:01:47.714893Z","iopub.status.idle":"2023-01-26T09:01:47.724928Z","shell.execute_reply.started":"2023-01-26T09:01:47.714867Z","shell.execute_reply":"2023-01-26T09:01:47.723974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class UpSampleConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        \n        self.upsample = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n        self.res_block = ResBlock(in_channels + out_channels, out_channels)\n        \n    def forward(self, inputs, skip):\n        x = self.upsample(inputs)\n        x = torch.cat([x, skip], dim=1)\n        x = self.res_block(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-01-26T09:01:47.726425Z","iopub.execute_input":"2023-01-26T09:01:47.726882Z","iopub.status.idle":"2023-01-26T09:01:47.735919Z","shell.execute_reply.started":"2023-01-26T09:01:47.726848Z","shell.execute_reply":"2023-01-26T09:01:47.73503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Generator(nn.Module):\n    def __init__(self, input_channel, output_channel, dropout_rate = 0.2):\n        super().__init__()\n        self.encoding_layer1_ = ResBlock(input_channel,64)\n        self.encoding_layer2_ = DownSampleConv(64, 128)\n        self.encoding_layer3_ = DownSampleConv(128, 256)\n        self.bridge = DownSampleConv(256, 512)\n        self.decoding_layer3_ = UpSampleConv(512, 256)\n        self.decoding_layer2_ = UpSampleConv(256, 128)\n        self.decoding_layer1_ = UpSampleConv(128, 64)\n        self.output = nn.Conv2d(64, output_channel, kernel_size=1)\n        self.dropout = nn.Dropout2d(dropout_rate)\n        \n    def forward(self, inputs):\n        ###################### Enocoder #########################\n        e1 = self.encoding_layer1_(inputs)\n        e1 = self.dropout(e1)\n        e2 = self.encoding_layer2_(e1)\n        e2 = self.dropout(e2)\n        e3 = self.encoding_layer3_(e2)\n        e3 = self.dropout(e3)\n        \n        ###################### Bridge #########################\n        bridge = self.bridge(e3)\n        bridge = self.dropout(bridge)\n        \n        ###################### Decoder #########################\n        d3 = self.decoding_layer3_(bridge, e3)\n        d2 = self.decoding_layer2_(d3, e2)\n        d1 = self.decoding_layer1_(d2, e1)\n        \n        ###################### Output #########################\n        output = self.output(d1)\n        return output","metadata":{"execution":{"iopub.status.busy":"2023-01-26T09:01:47.737042Z","iopub.execute_input":"2023-01-26T09:01:47.737998Z","iopub.status.idle":"2023-01-26T09:01:47.748661Z","shell.execute_reply.started":"2023-01-26T09:01:47.73797Z","shell.execute_reply":"2023-01-26T09:01:47.747603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Generator(1,2).to(device)\nsummary(model, (1, 224, 224), batch_size = 1)","metadata":{"execution":{"iopub.status.busy":"2023-01-26T09:01:47.750033Z","iopub.execute_input":"2023-01-26T09:01:47.750485Z","iopub.status.idle":"2023-01-26T09:01:47.880567Z","shell.execute_reply.started":"2023-01-26T09:01:47.75044Z","shell.execute_reply":"2023-01-26T09:01:47.879527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# B. Discriminator ( Critic )","metadata":{}},{"cell_type":"markdown","source":"## 1. Theory","metadata":{}},{"cell_type":"markdown","source":"The class Critic in this research is a crucial component of the proposed architecture for image recoloring using a conditional WGAN. This class defines the architecture of the critic network, which is responsible for evaluating the quality of the generated images. The critic network is trained to differentiate between real and fake images, where the real images are the ground truth images in the LAB color space, and the fake images are the generated images by the generator network.","metadata":{}},{"cell_type":"markdown","source":"## 2. Architecture","metadata":{}},{"cell_type":"markdown","source":"The architecture of the critic network follows a standard convolutional neural network (CNN) design, where the input image is processed through a series of convolutional layers followed by batch normalization, LeakyReLU activation, and downsampling layers. The convolutional layers are designed to extract features from the input image, and the downsampling layers are responsible for reducing the spatial dimensions of the feature maps while increasing the number of filters.\n\nThe architecture of the critic network is designed to handle the input images in the LAB color space, where the input image is the concatenation of the ab channels and the L channel. The output of the critic network is a scalar value, representing the probability of the input image being real or fake.\n\nThe proposed architecture of the critic network is important for image recoloring task, as it allows the generator network to learn the underlying distribution of the real images in the LAB color space. By providing a reliable evaluation of the generated images, the critic network helps the generator network to produce more realistic and high-quality images. Additionally, the use of the LeakyReLU activation function and the Instance Normalization layers improve the performance of the critic network, as they help to stabilize the training process and reduce the mode collapse problem.","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\" style=\"font-size:14px; color:black; background-color:#ffeee6; border-color: #ff661a\">\n    <aside>\nüìô  In our project, The discriminator network is composed of 5 convolutional layers, each followed by a Intestance normalization layer and a LeakyReLU activation function. The first 4 layers have a stride of 2, which reduces the spatial resolution of the feature map by a factor of 4. The last layer is a fully connected layer that outputs a single output representing the probability that a patch is real. ","metadata":{}},{"cell_type":"markdown","source":"<center> <img src=\"https://i.imgur.com/rG6DjQA.png\"></center>","metadata":{}},{"cell_type":"markdown","source":"## 2. Implementation","metadata":{}},{"cell_type":"code","source":"class Critic(nn.Module):\n    def __init__(self, in_channels=3):\n        super(Critic, self).__init__()\n\n        def critic_block(in_filters, out_filters, normalization=True):\n            \"\"\"Returns layers of each critic block\"\"\"\n            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n            if normalization:\n                layers.append(nn.InstanceNorm2d(out_filters))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n\n        self.model = nn.Sequential(\n            *critic_block(in_channels, 64, normalization=False),\n            *critic_block(64, 128),\n            *critic_block(128, 256),\n            *critic_block(256, 512),\n            nn.AdaptiveAvgPool2d(1),\n            nn.Flatten(),\n            nn.Linear(512, 1)\n        )\n\n    def forward(self, ab, l):\n        # Concatenate image and condition image by channels to produce input\n        img_input = torch.cat((ab, l), 1)\n        output = self.model(img_input)\n        return output","metadata":{"execution":{"iopub.status.busy":"2023-01-26T09:01:47.882107Z","iopub.execute_input":"2023-01-26T09:01:47.882418Z","iopub.status.idle":"2023-01-26T09:01:47.891449Z","shell.execute_reply.started":"2023-01-26T09:01:47.882387Z","shell.execute_reply":"2023-01-26T09:01:47.89025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\" style=\"font-size:14px; color:black; background-color:#e6ffe6; border-color: #1aff1a\">\n    <aside>\nüìó The forward method takes in two inputs, \"ab\" and \"l\", concatenates them along the channel dimension and pass them through the model. The model is a sequential of convolutional layers, Instance normalization, Leaky ReLU activation and AdaptiveAvgPool2d, Flatten and Linear layers. The output of the forward method is a single scalar value representing the Wasserstein distance between the true and fake data distributions.","metadata":{}},{"cell_type":"code","source":"model = Critic(3).to(device)\nsummary(model, [(2, 224, 224), (1, 224, 224)], batch_size = 1)","metadata":{"execution":{"iopub.status.busy":"2023-01-26T09:01:47.893254Z","iopub.execute_input":"2023-01-26T09:01:47.894046Z","iopub.status.idle":"2023-01-26T09:01:47.938457Z","shell.execute_reply.started":"2023-01-26T09:01:47.894012Z","shell.execute_reply":"2023-01-26T09:01:47.937503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# C. Generative Adversarial Network","metadata":{}},{"cell_type":"markdown","source":"## 1. Theory","metadata":{}},{"cell_type":"markdown","source":"WGAN (Wasserstein Generative Adversarial Network) is a type of GAN (Generative Adversarial Network) that uses the Wasserstein distance as the loss function for the generator and the critic. The Wasserstein distance, also known as the Earth Mover's distance, is a distance metric that measures the amount of \"work\" required to transform one probability distribution into another.\n\nIn a traditional GAN, the generator and the critic are trained to minimize the Jensen-Shannon divergence, which is a measure of the difference between two probability distributions. However, the Jensen-Shannon divergence can be difficult to optimize and can lead to instability in the training process.\n\nThe WGAN addresses this issue by using the Wasserstein distance as the loss function, which allows for more stable training of the generator and the critic. Additionally, the Wasserstein distance has a nice property that the gradient of the loss function is always well defined, which allows for more stable optimization.\n\nIn WGAN, the critic is trained to approximate the Wasserstein distance between the real and fake data distributions. The generator is trained to generate samples that will cause the critic to output low values. In WGAN, the critic network is trained to be a 1-Lipschitz function, which means that the critic's output should change by at most 1 for any small change in the input.\n\nOverall, WGAN is a more stable and well-defined version of GAN, that allows for more control over the training process and can help to improve the quality and diversity of the generated samples.","metadata":{}},{"cell_type":"code","source":"# https://stackoverflow.com/questions/49433936/how-to-initialize-weights-in-pytorch\ndef _weights_init(m):\n    if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n    if isinstance(m, nn.BatchNorm2d):\n        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n        torch.nn.init.constant_(m.bias, 0)","metadata":{"execution":{"iopub.status.busy":"2023-01-26T09:01:47.939863Z","iopub.execute_input":"2023-01-26T09:01:47.940198Z","iopub.status.idle":"2023-01-26T09:01:47.947099Z","shell.execute_reply.started":"2023-01-26T09:01:47.940165Z","shell.execute_reply":"2023-01-26T09:01:47.94598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def display_progress(cond, real, fake, current_epoch = 0, figsize=(20,15)):\n    \"\"\"\n    Save cond, real (original) and generated (fake)\n    images in one panel \n    \"\"\"\n    cond = cond.detach().cpu().permute(1, 2, 0)   \n    real = real.detach().cpu().permute(1, 2, 0)\n    fake = fake.detach().cpu().permute(1, 2, 0)\n    \n    images = [cond, real, fake]\n    titles = ['input','real','generated']\n    print(f'Epoch: {current_epoch}')\n    fig, ax = plt.subplots(1, 3, figsize=figsize)\n    for idx,img in enumerate(images):\n        if idx == 0:\n            ab = torch.zeros((224,224,2))\n            img = torch.cat([images[0]* 100, ab], dim=2).numpy()\n            imgan = lab2rgb(img)\n        else:\n            imgan = lab_to_rgb(images[0],img)\n        ax[idx].imshow(imgan)\n        ax[idx].axis(\"off\")\n    for idx, title in enumerate(titles):    \n        ax[idx].set_title('{}'.format(title))\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-01-26T09:01:47.948778Z","iopub.execute_input":"2023-01-26T09:01:47.949312Z","iopub.status.idle":"2023-01-26T09:01:47.959665Z","shell.execute_reply.started":"2023-01-26T09:01:47.949279Z","shell.execute_reply":"2023-01-26T09:01:47.958532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CWGAN(pl.LightningModule):\n\n    def __init__(self, in_channels, out_channels, learning_rate=0.0002, lambda_recon=100, display_step=10, lambda_gp=10, lambda_r1=10,):\n\n        super().__init__()\n        self.save_hyperparameters()\n        \n        self.display_step = display_step\n        \n        self.generator = Generator(in_channels, out_channels)\n        self.critic = Critic(in_channels + out_channels)\n        self.optimizer_G = optim.Adam(self.generator.parameters(), lr=learning_rate, betas=(0.5, 0.9))\n        self.optimizer_C = optim.Adam(self.critic.parameters(), lr=learning_rate, betas=(0.5, 0.9))\n        self.lambda_recon = lambda_recon\n        self.lambda_gp = lambda_gp\n        self.lambda_r1 = lambda_r1\n        self.recon_criterion = nn.L1Loss()\n        self.generator_losses, self.critic_losses  =[],[]\n    \n    def configure_optimizers(self):\n        return [self.optimizer_C, self.optimizer_G]\n        \n    def generator_step(self, real_images, conditioned_images):\n        # WGAN has only a reconstruction loss\n        self.optimizer_G.zero_grad()\n        fake_images = self.generator(conditioned_images)\n        recon_loss = self.recon_criterion(fake_images, real_images)\n        recon_loss.backward()\n        self.optimizer_G.step()\n        \n        # Keep track of the average generator loss\n        self.generator_losses += [recon_loss.item()]\n        \n        \n    def critic_step(self, real_images, conditioned_images):\n        self.optimizer_C.zero_grad()\n        fake_images = self.generator(conditioned_images)\n        fake_logits = self.critic(fake_images, conditioned_images)\n        real_logits = self.critic(real_images, conditioned_images)\n        \n        # Compute the loss for the critic\n        loss_C = real_logits.mean() - fake_logits.mean()\n\n        # Compute the gradient penalty\n        alpha = torch.rand(real_images.size(0), 1, 1, 1, requires_grad=True)\n        alpha = alpha.to(device)\n        interpolated = (alpha * real_images + (1 - alpha) * fake_images.detach()).requires_grad_(True)\n        \n        interpolated_logits = self.critic(interpolated, conditioned_images)\n        \n        grad_outputs = torch.ones_like(interpolated_logits, dtype=torch.float32, requires_grad=True)\n        gradients = torch.autograd.grad(outputs=interpolated_logits, inputs=interpolated, grad_outputs=grad_outputs,create_graph=True, retain_graph=True)[0]\n\n        \n        gradients = gradients.view(len(gradients), -1)\n        gradients_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n        loss_C += self.lambda_gp * gradients_penalty\n        \n        # Compute the R1 regularization loss\n        r1_reg = gradients.pow(2).sum(1).mean()\n        loss_C += self.lambda_r1 * r1_reg\n\n        # Backpropagation\n        loss_C.backward()\n        self.optimizer_C.step()\n        self.critic_losses += [loss_C.item()]\n        \n    def training_step(self, batch, batch_idx, optimizer_idx):\n        real, condition = batch\n        if optimizer_idx == 0:\n            self.critic_step(real, condition)\n        elif optimizer_idx == 1:\n            self.generator_step(real, condition)\n        gen_mean = sum(self.generator_losses[-self.display_step:]) / self.display_step\n        crit_mean = sum(self.critic_losses[-self.display_step:]) / self.display_step\n        if self.current_epoch%self.display_step==0 and batch_idx==0 and optimizer_idx==1:\n            fake = self.generator(condition).detach()\n            torch.save(cwgan.generator.state_dict(), \"ResUnet_\"+ str(self.current_epoch) +\".pt\")\n            torch.save(cwgan.critic.state_dict(), \"PatchGAN_\"+ str(self.current_epoch) +\".pt\")\n            print(f\"Epoch {self.current_epoch} : Generator loss: {gen_mean}, Critic loss: {crit_mean}\")\n            display_progress(condition[0], real[0], fake[0], self.current_epoch)\n","metadata":{"execution":{"iopub.status.busy":"2023-01-26T09:01:47.961644Z","iopub.execute_input":"2023-01-26T09:01:47.962204Z","iopub.status.idle":"2023-01-26T09:01:47.981457Z","shell.execute_reply.started":"2023-01-26T09:01:47.962171Z","shell.execute_reply":"2023-01-26T09:01:47.980498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()\ncwgan = CWGAN(in_channels = 1, out_channels = 2 ,learning_rate=2e-4, lambda_recon=100, display_step=10)","metadata":{"execution":{"iopub.status.busy":"2023-01-26T09:01:47.983913Z","iopub.execute_input":"2023-01-26T09:01:47.984233Z","iopub.status.idle":"2023-01-26T09:01:48.283085Z","shell.execute_reply.started":"2023-01-26T09:01:47.984187Z","shell.execute_reply":"2023-01-26T09:01:48.282112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer = pl.Trainer(max_epochs=150, gpus=-1)\ntrainer.fit(cwgan, train_loader)","metadata":{"execution":{"iopub.status.busy":"2023-01-26T09:30:27.913Z","iopub.execute_input":"2023-01-26T09:30:27.913403Z","iopub.status.idle":"2023-01-26T09:43:30.397647Z","shell.execute_reply.started":"2023-01-26T09:30:27.913367Z","shell.execute_reply":"2023-01-26T09:43:30.396686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<hr>\n<h1><center>VI. Model Inferencing</center></h1>\n<hr>","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(30,60))\nidx =1\nfor batch_idx, batch in enumerate(test_loader):\n    real, condition = batch\n    pred = cwgan.generator(condition).detach().squeeze().permute(1, 2, 0)\n    condition  = condition.detach().squeeze(0).permute(1, 2, 0)\n    real  = real.detach().squeeze(0).permute(1, 2, 0)\n    plt.subplots_adjust(wspace=0, hspace=0)\n    plt.subplot(6,3,idx)\n    plt.grid(False)\n    \n    ab = torch.zeros((224,224,2))\n    img = torch.cat([condition * 100, ab], dim=2).numpy()\n    imgan = lab2rgb(img)\n    plt.imshow(imgan)\n    plt.title('Input')\n    \n    plt.subplot(6,3,idx + 1)\n    \n    ab = torch.zeros((224,224,2))\n    imgan = lab_to_rgb(condition,real)\n    plt.imshow(imgan)\n    plt.title('Real')\n    \n    plt.subplot(6,3,idx + 2)\n    imgan = lab_to_rgb(condition,pred)\n    plt.title('Generated')\n    plt.imshow(imgan)\n    idx += 3\n    if idx >= 18:\n        break","metadata":{"execution":{"iopub.status.busy":"2023-01-26T09:47:39.394737Z","iopub.execute_input":"2023-01-26T09:47:39.395107Z","iopub.status.idle":"2023-01-26T09:47:50.896368Z","shell.execute_reply.started":"2023-01-26T09:47:39.39507Z","shell.execute_reply":"2023-01-26T09:47:50.895147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<hr>\n<h1><center>VI. Evaluation</center></h1>\n<hr>","metadata":{}},{"cell_type":"markdown","source":"## A. Measuring GAN Performance\n\n### 1. Human scoring\nOne way to evaluate the performance of a GAN is to use human scoring, where real images and images created by the GAN are randomly stacked together, and human scorers label each image as real or fake. This can be done using platforms such as Amazon Mechanical Turk.\n\n### 2. Inception Score (IS)\nAnother way to evaluate GAN performance is to use the Inception Score, which uses a pre-trained Inception model to classify the generated images. The IS considers both the quality and diversity of the generated images by measuring the entropy of the classification labels. The higher the entropy, the more diverse and unpredictable the generated images are, indicating a better performance.\n\n<center><img width = \"700px\" src=\"https://cdn-images-1.medium.com/max/1600/1*RdIYRsqXxRAKwcjtxg6_kw.jpeg\"></center>\n\n\n### 3. Frechet Inception Distance (FID)\nIn Frechet Inception Distance (FID), we utilize the Inception network to extract features from an intermediate layer. Then, we model the data distribution for these features using a multivariate Gaussian distribution with mean $\\mu$ and covariance $\\Sigma$. The FID between the real images $x$ and generated images $g$ is computed as: \\\\\n\n\n$FID(x,g) = \\left\\|\\mu_x - \\mu_g\\right\\|^2_2 + Tr(\\Sigma_x + \\Sigma_g - 2(\\Sigma_x\\Sigma_g)^{\\frac{1}{2}})$ \n\n\nwhere $Tr$ sums up all the diagonal elements. Lower FID values indicate better image quality and diversity.\n\nFID is sensitive to mode collapse, meaning that as the distance between simulated missing modes increases, the FID value will also increase. Additionally, FID is more robust to noise than Inception Score (IS) as it is less affected by models that only generate one image per class. Therefore, FID is a better measurement for image diversity. However, it should be noted that FID has some rather high bias but low variance. By computing the FID between a training dataset and a testing dataset, we should expect the FID to be zero since both are real images. However, running the test with different batches of training sample shows non-zero FID.\n","metadata":{}},{"cell_type":"markdown","source":"## B. Implemenation of Inception Score","metadata":{}},{"cell_type":"code","source":"# disable grads + batchnorm + dropout\ntorch.set_grad_enabled(False)\ncwgan.generator.eval()\nall_preds = []\nall_real = []\n\nfor batch_idx, batch in enumerate(test_loader):\n    real, condition = batch\n    pred = cwgan.generator(condition).detach()\n    Lab = torch.cat([condition, pred], dim=1).numpy()\n    Lab_real = torch.cat([condition, real], dim=1).numpy()\n    all_preds.append(Lab.squeeze())\n    all_real.append(Lab_real.squeeze())\n    if batch_idx == 500: break","metadata":{"execution":{"iopub.status.busy":"2023-01-26T09:28:05.921991Z","iopub.execute_input":"2023-01-26T09:28:05.922574Z","iopub.status.idle":"2023-01-26T09:29:15.929173Z","shell.execute_reply.started":"2023-01-26T09:28:05.922538Z","shell.execute_reply":"2023-01-26T09:29:15.928166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class InceptionScore:\n    def __init__(self, device):\n        self.device = device\n        self.inception = inception_v3(pretrained=True, transform_input=False).to(self.device)\n        self.inception.eval()\n\n    def calculate_is(self, generated_images):\n        generated_images = generated_images.to(self.device)\n\n        with torch.no_grad():\n            generated_features = self.inception(generated_images.view(-1,3,224,224))\n\n        generated_features = generated_features.view(generated_features.size(0), -1)\n        p = F.softmax(generated_features, dim=1)\n\n        kl = p * (torch.log(p) - torch.log(torch.tensor(1.0/generated_features.size(1)).to(self.device)))\n        kl = kl.sum(dim=1)\n\n        return kl.mean().item(), kl.std().item()","metadata":{"execution":{"iopub.status.busy":"2023-01-26T09:29:15.930589Z","iopub.execute_input":"2023-01-26T09:29:15.931048Z","iopub.status.idle":"2023-01-26T09:29:15.939536Z","shell.execute_reply.started":"2023-01-26T09:29:15.931009Z","shell.execute_reply":"2023-01-26T09:29:15.93852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize the InceptionScore class\ndevice = \"cuda\" # or \"cpu\" if you don't have a GPU\nis_calculator = InceptionScore(device)\n\nall_preds = np.concatenate(all_preds, axis=0)\nall_preds = torch.tensor(all_preds).float()\n\nall_real = np.concatenate(all_real, axis=0)\nall_real = torch.tensor(all_real).float()\n\nis_model = InceptionScore(device)\n\n# Calculate the Inception Score\nmean_real, std_real = is_model.calculate_is(all_real)\nprint(\"Inception Score of real images: mean: {:.4f}, std: {:.4f}\".format(mean_real, std_real))\nmean_is, std_is = is_model.calculate_is(all_preds)\nprint(\"Inception Score of fake images: mean: {:.4f}, std: {:.4f}\".format(mean_is, std_is))","metadata":{"execution":{"iopub.status.busy":"2023-01-26T09:29:15.941146Z","iopub.execute_input":"2023-01-26T09:29:15.941819Z","iopub.status.idle":"2023-01-26T09:29:17.90228Z","shell.execute_reply.started":"2023-01-26T09:29:15.941775Z","shell.execute_reply":"2023-01-26T09:29:17.901207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## B. Implemenation of Fr√©chet Inception Distance (FID)","metadata":{}},{"cell_type":"code","source":"class FID:\n    def __init__(self, device):\n        self.device = device\n        self.inception = inception_v3(pretrained=True, transform_input=False).to(self.device)\n        self.inception.eval()\n        self.mu = None\n        self.sigma = None\n\n    def calculate_fid(self, real_images, generated_images):\n        real_images = real_images.to(self.device)\n        generated_images = generated_images.to(self.device)\n\n        with torch.no_grad():\n            real_features = self.inception(real_images.view(-1,3,224,224))\n            generated_features = self.inception(generated_images.view(-1,3,224,224))\n\n        real_features = real_features.view(real_features.size(0), -1)\n        generated_features = generated_features.view(generated_features.size(0), -1)\n\n        if self.mu is None:\n            self.mu = real_features.mean(dim=0)\n\n        if self.sigma is None:\n            self.sigma = real_features.std(dim=0)\n\n        real_mu = real_features.mean(dim=0)\n        real_sigma = real_features.std(dim=0)\n\n        generated_mu = generated_features.mean(dim=0)\n        generated_sigma = generated_features.std(dim=0)\n\n        mu_diff = real_mu - generated_mu\n        sigma_diff = real_sigma - generated_sigma\n\n        fid = mu_diff.pow(2).sum() + (self.sigma - generated_sigma).pow(2).sum() + (self.mu - generated_mu).pow(2).sum()\n        return fid.item()","metadata":{"execution":{"iopub.status.busy":"2023-01-26T09:29:17.903873Z","iopub.execute_input":"2023-01-26T09:29:17.904322Z","iopub.status.idle":"2023-01-26T09:29:17.914798Z","shell.execute_reply.started":"2023-01-26T09:29:17.904284Z","shell.execute_reply":"2023-01-26T09:29:17.913702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize the FID class\ndevice = \"cuda\" # or \"cpu\" if you don't have a GPU\nfid_calculator = FID(device)\n\n# Calculate the FID\nfid_value = fid_calculator.calculate_fid(all_real, all_preds)\nprint(\"FID: {:.4f}\".format(fid_value))","metadata":{"execution":{"iopub.status.busy":"2023-01-26T09:29:17.916813Z","iopub.execute_input":"2023-01-26T09:29:17.917573Z","iopub.status.idle":"2023-01-26T09:29:18.641987Z","shell.execute_reply.started":"2023-01-26T09:29:17.917538Z","shell.execute_reply":"2023-01-26T09:29:18.640825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<hr>\n<h1><center>VII. Conclusion</center></h1>\n<hr>","metadata":{}},{"cell_type":"markdown","source":"\n<div class=\"alert alert-block alert-info\" style=\"font-size:14px; color:black; background-color:#e6f2ff; border-color: #3399ff\">\n    <aside>\n    üìò In conclusion, Pix2pix is a powerful model for image-to-image translation tasks, but it can be further improved for specific applications such as colorization. Using Wasserstein GAN (WGAN) and a U-Net architecture based on residual blocks are two ways to improve the performance of pix2pix for colorization. WGANs use the Wasserstein distance metric to train the generator and discriminator which can help stabilize the training process and produce more realistic results. A U-Net architecture which is well-suited for image segmentation tasks combined with Residual blocks allows the network to learn fine details of the input image, which can be particularly useful for colorization tasks. This can help improve the stability and the ability to learn fine details of the input image, producing more realistic results.\n    </aside>\n</div>","metadata":{}}]}